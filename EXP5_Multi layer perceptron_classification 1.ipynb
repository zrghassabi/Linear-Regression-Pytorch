{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: torch.Size([600, 2]) x_validation.shape: torch.Size([150, 2]) x_test.shape: torch.Size([250, 2])\n"
     ]
    }
   ],
   "source": [
    "# MLP by Rozita 9/16/2021\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  #replace all torch.nn as nn\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "\n",
    "#step 1: generate Data \n",
    "\n",
    "# 1-1 first approach \n",
    "#x=torch.randn(100,3)  #3D\n",
    "#yt=torch.randn(100,2) #Label, 2D\n",
    "#print(x.shape)\n",
    "#x=torch.FloatTensor(x)     #do not forget to convert data from numpyarray to tensor\n",
    "                            #or use torch.tensor(x).float()\n",
    "#yt=torch.LongTensor(yt)    #or use torch.tensor(yt).long()\n",
    "\n",
    "\n",
    "# 1-2 you can generate data in this way, too\n",
    "#n_features means x,y , our data and Labels\n",
    "#if you get same data in each iteration use random_state=1\n",
    "num_features=2\n",
    "num_classes=3\n",
    "num_samples=1000\n",
    "\n",
    "x, yt = make_blobs(n_samples=num_samples, n_features=num_features, centers=num_classes, cluster_std=1.0, random_state=1)\n",
    "\n",
    "#plotting Data\n",
    "for class_value in range(num_classes):\n",
    "    row_idx = where(yt == class_value)\n",
    "    pyplot.scatter(x[row_idx, 0], x[row_idx, 1])\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# split samples into three sets: Train, Validation, Test\n",
    "# Later you can change this part for K-fold cross-validation -------------------------------\n",
    "x_train= torch.tensor(x[:int(0.6*num_samples),:]).float()\n",
    "y_train= torch.tensor(yt[:int(0.6*num_samples)]).long()\n",
    "\n",
    "x_valid=torch.tensor(x[int(0.6*num_samples):int(0.75*num_samples),:]).float()\n",
    "y_valid=torch.tensor(yt[int(0.6*num_samples):int(0.75*num_samples)]).long()\n",
    "\n",
    "#x_test=torch.tensor(x[int(0.75*num_samples):num_samples,:]).float()  \n",
    "#or\n",
    "x_test = torch.tensor(x[int(0.75*num_samples):, :]).float()\n",
    "y_test=torch.tensor(yt[int(0.75*num_samples)]).long()\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "print ('x_train.shape:', x_train.shape, 'x_validation.shape:', x_valid.shape, 'x_test.shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=3, bias=True)\n",
      ") \n",
      " Loss: CrossEntropyLoss() \n",
      " optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# step 2) Define Model, Loss, Optimizer ------------------------------------------------------------------------\n",
    "# 2-1)creat a Model\n",
    "# 2-1-1) In case of single layer:\n",
    "#model=torch.nn.Linear(num_features,num_classes)  #input No. is 3 and output No. is 2\n",
    "# 2-1-2) In case of multi-layer:\n",
    "\n",
    "num_hidden1=10;\n",
    "model = nn.Sequential(\n",
    "                          nn.Linear(num_features,num_hidden1),\n",
    "                          nn.ReLU(),                           #see other activation functions in help of pytorch\n",
    "                          nn.Linear(num_hidden1, num_classes)\n",
    "                         )\n",
    "\n",
    "# 2-2) Loss Function\n",
    "# MSE loss not good idea for classification, use pytorch.org in doc, see torch.nn and in help look for loss\n",
    "loss = nn.CrossEntropyLoss()  #since cross-entropy has softmax, we did not use softmax in the model definition in step 2-1\n",
    "\n",
    "# 2-3) optimizer\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.001) #lr is Learning rate\n",
    "\n",
    "print('Model:',model,'\\n Loss:', loss, '\\n optimizer:', optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss_value: 0.23506437242031097 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 1 loss_value: 0.23470042645931244 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 2 loss_value: 0.23433749377727509 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 3 loss_value: 0.23397588729858398 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 4 loss_value: 0.2336151897907257 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 5 loss_value: 0.2332555651664734 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 6 loss_value: 0.23289726674556732 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 7 loss_value: 0.23253999650478363 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 8 loss_value: 0.23218384385108948 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 9 loss_value: 0.2318286895751953 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 10 loss_value: 0.23147457838058472 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 11 loss_value: 0.2311214953660965 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 12 loss_value: 0.23076938092708588 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 13 loss_value: 0.23041829466819763 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 14 loss_value: 0.2300683557987213 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 15 loss_value: 0.22971923649311066 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 16 loss_value: 0.22937148809432983 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 17 loss_value: 0.22902455925941467 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 18 loss_value: 0.22867894172668457 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 19 loss_value: 0.2283342480659485 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 20 loss_value: 0.22799044847488403 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 21 loss_value: 0.2276478111743927 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 22 loss_value: 0.2273062914609909 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 23 loss_value: 0.22696568071842194 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 24 loss_value: 0.22662627696990967 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 25 loss_value: 0.22628754377365112 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 26 loss_value: 0.2259499877691269 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 27 loss_value: 0.22561368346214294 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 28 loss_value: 0.22527816891670227 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 29 loss_value: 0.22494354844093323 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 30 loss_value: 0.2246098816394806 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 31 loss_value: 0.22427746653556824 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 32 loss_value: 0.2239459753036499 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 33 loss_value: 0.2236153483390808 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 34 loss_value: 0.2232859581708908 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 35 loss_value: 0.22295725345611572 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 36 loss_value: 0.22262977063655853 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 37 loss_value: 0.22230295836925507 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 38 loss_value: 0.2219770848751068 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 39 loss_value: 0.22165226936340332 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 40 loss_value: 0.22132833302021027 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 41 loss_value: 0.2210053950548172 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 42 loss_value: 0.22068333625793457 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 43 loss_value: 0.22036221623420715 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 44 loss_value: 0.22004203498363495 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 45 loss_value: 0.2197228968143463 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 46 loss_value: 0.2194046825170517 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 47 loss_value: 0.2190871685743332 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 48 loss_value: 0.21877072751522064 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 49 loss_value: 0.2184552699327469 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 50 loss_value: 0.2181406468153 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 51 loss_value: 0.21782688796520233 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 52 loss_value: 0.2175142467021942 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 53 loss_value: 0.21720241010189056 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 54 loss_value: 0.2168913036584854 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 55 loss_value: 0.21658124029636383 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 56 loss_value: 0.2162720263004303 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 57 loss_value: 0.2159636914730072 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 58 loss_value: 0.21565614640712738 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 59 loss_value: 0.21534965932369232 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 60 loss_value: 0.21504394710063934 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 61 loss_value: 0.21473897993564606 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 62 loss_value: 0.21443524956703186 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 63 loss_value: 0.21413202583789825 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 64 loss_value: 0.21382994949817657 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 65 loss_value: 0.2135285884141922 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 66 loss_value: 0.21322806179523468 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 67 loss_value: 0.2129283845424652 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 68 loss_value: 0.21262948215007782 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 69 loss_value: 0.2123316377401352 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 70 loss_value: 0.2120344489812851 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 71 loss_value: 0.21173816919326782 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 72 loss_value: 0.21144269406795502 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 73 loss_value: 0.21114808320999146 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 74 loss_value: 0.21085432171821594 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 75 loss_value: 0.21056123077869415 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 76 loss_value: 0.21026907861232758 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 77 loss_value: 0.20997777581214905 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 78 loss_value: 0.209687277674675 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 79 loss_value: 0.20939753949642181 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 80 loss_value: 0.20910856127738953 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 81 loss_value: 0.20882059633731842 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 82 loss_value: 0.20853322744369507 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 83 loss_value: 0.20824666321277618 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 84 loss_value: 0.2079610526561737 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 85 loss_value: 0.20767594873905182 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 86 loss_value: 0.2073918730020523 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 87 loss_value: 0.2071085423231125 Train accuracy: 0.5950000286102295 Validation accuracy: 0.14900000393390656\n",
      "epoch: 88 loss_value: 0.20682589709758759 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 89 loss_value: 0.20654422044754028 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 90 loss_value: 0.20626306533813477 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 91 loss_value: 0.2059830278158188 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 92 loss_value: 0.20570336282253265 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 93 loss_value: 0.20542468130588531 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 94 loss_value: 0.2051466703414917 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 95 loss_value: 0.20486949384212494 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 96 loss_value: 0.20459303259849548 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 97 loss_value: 0.2043173611164093 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 98 loss_value: 0.20404238998889923 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 99 loss_value: 0.20376832783222198 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 100 loss_value: 0.20349472761154175 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 101 loss_value: 0.2032221257686615 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 102 loss_value: 0.20295001566410065 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 103 loss_value: 0.20267879962921143 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 104 loss_value: 0.20240819454193115 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 105 loss_value: 0.2021385133266449 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 106 loss_value: 0.20186945796012878 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 107 loss_value: 0.20160116255283356 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 108 loss_value: 0.20133350789546967 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 109 loss_value: 0.20106655359268188 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 110 loss_value: 0.20080047845840454 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 111 loss_value: 0.2005348801612854 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 112 loss_value: 0.20027010142803192 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 113 loss_value: 0.2000061422586441 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 114 loss_value: 0.19974276423454285 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 115 loss_value: 0.1994801014661789 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 116 loss_value: 0.19921809434890747 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 117 loss_value: 0.19895686209201813 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 118 loss_value: 0.19869625568389893 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 119 loss_value: 0.1984361708164215 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 120 loss_value: 0.19817711412906647 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 121 loss_value: 0.19791853427886963 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 122 loss_value: 0.19766078889369965 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 123 loss_value: 0.197403684258461 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 124 loss_value: 0.19714705646038055 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 125 loss_value: 0.19689130783081055 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 126 loss_value: 0.19663605093955994 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 127 loss_value: 0.196381613612175 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 128 loss_value: 0.196127787232399 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 129 loss_value: 0.19587455689907074 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 130 loss_value: 0.19562223553657532 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 131 loss_value: 0.19537019729614258 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 132 loss_value: 0.19511905312538147 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 133 loss_value: 0.19486849009990692 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 134 loss_value: 0.19461877644062042 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 135 loss_value: 0.19436949491500854 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 136 loss_value: 0.19412082433700562 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 137 loss_value: 0.19387297332286835 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 138 loss_value: 0.1936255395412445 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 139 loss_value: 0.1933789849281311 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 140 loss_value: 0.19313299655914307 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 141 loss_value: 0.1928875744342804 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 142 loss_value: 0.1926427185535431 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 143 loss_value: 0.19239871203899384 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 144 loss_value: 0.19215518236160278 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 145 loss_value: 0.1919122189283371 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 146 loss_value: 0.19167008996009827 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 147 loss_value: 0.19142840802669525 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 148 loss_value: 0.19118735194206238 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 149 loss_value: 0.19094693660736084 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 150 loss_value: 0.19070707261562347 Train accuracy: 0.5960000157356262 Validation accuracy: 0.14900000393390656\n",
      "epoch: 151 loss_value: 0.19046790897846222 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 152 loss_value: 0.19022931158542633 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 153 loss_value: 0.18999135494232178 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 154 loss_value: 0.1897539496421814 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 155 loss_value: 0.18951719999313354 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 156 loss_value: 0.18928104639053345 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 157 loss_value: 0.18904542922973633 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 158 loss_value: 0.18881061673164368 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 159 loss_value: 0.18857596814632416 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 160 loss_value: 0.18834228813648224 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 161 loss_value: 0.18810898065567017 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 162 loss_value: 0.1878764033317566 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 163 loss_value: 0.18764421343803406 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 164 loss_value: 0.18741273880004883 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 165 loss_value: 0.18718194961547852 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 166 loss_value: 0.18695157766342163 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 167 loss_value: 0.1867218166589737 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 168 loss_value: 0.18649274110794067 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 169 loss_value: 0.18626408278942108 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 170 loss_value: 0.18603603541851044 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 171 loss_value: 0.18580865859985352 Train accuracy: 0.597000002861023 Validation accuracy: 0.14900000393390656\n",
      "epoch: 172 loss_value: 0.1855817437171936 Train accuracy: 0.597000002861023 Validation accuracy: 0.15000000596046448\n",
      "epoch: 173 loss_value: 0.18535538017749786 Train accuracy: 0.597000002861023 Validation accuracy: 0.15000000596046448\n",
      "epoch: 174 loss_value: 0.18512947857379913 Train accuracy: 0.597000002861023 Validation accuracy: 0.15000000596046448\n",
      "epoch: 175 loss_value: 0.18490439653396606 Train accuracy: 0.597000002861023 Validation accuracy: 0.15000000596046448\n",
      "epoch: 176 loss_value: 0.18467977643013 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 177 loss_value: 0.18445563316345215 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 178 loss_value: 0.18423213064670563 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 179 loss_value: 0.18400898575782776 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 180 loss_value: 0.18378664553165436 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 181 loss_value: 0.18356461822986603 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 182 loss_value: 0.18334320187568665 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 183 loss_value: 0.1831224262714386 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 184 loss_value: 0.18290217220783234 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 185 loss_value: 0.18268246948719025 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 186 loss_value: 0.18246300518512726 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 187 loss_value: 0.1822444349527359 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 188 loss_value: 0.18202629685401917 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 189 loss_value: 0.18180860579013824 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 190 loss_value: 0.1815914511680603 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 191 loss_value: 0.18137510120868683 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 192 loss_value: 0.18115897476673126 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 193 loss_value: 0.1809433251619339 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 194 loss_value: 0.18072833120822906 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 195 loss_value: 0.18051376938819885 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 196 loss_value: 0.18029989302158356 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 197 loss_value: 0.1800863891839981 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 198 loss_value: 0.17987342178821564 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "epoch: 199 loss_value: 0.17966097593307495 Train accuracy: 0.5979999899864197 Validation accuracy: 0.15000000596046448\n",
      "\n",
      " Test accuracy: 0.08799999952316284\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "num_epochs=200\n",
    "for epoch in range(num_epochs):\n",
    "    # this is necessary to reset gradients (write model.bias.grad in command window when running breakpoint)\n",
    "    optimizer.zero_grad()\n",
    "    yp = model(x_train)\n",
    "    loss_value=loss(yp,y_train)\n",
    "    # (yp is 1000*3), torch.max, finds maximum in each sample\n",
    "    # a includes two values: indices and values\n",
    "    # use breakpoint and check a=torch.max(yp,1) and a[0]==yt , 0 indicates incorrect and 1 indicates correct\n",
    "    #  or a=torch.argmax(yp,1)\n",
    "\n",
    "    num_corrects=torch.sum(torch.max(yp,1)[1]==y_train) #use breakpoint\n",
    "    acc_train=num_corrects.float()/num_samples.__float__()  #use breakpoint\n",
    "    # or acc=num_corrects.float()/float(num_samples)\n",
    "\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    #model.weights.grad\n",
    "    #model.bias.grad\n",
    "\n",
    "    yp=model(x_valid) # for validation you do not need loss_value.backward\n",
    "    num_corrects = torch.sum(torch.max(yp, 1)[1] == y_valid)  # use breakpoint\n",
    "    acc_valid = num_corrects.float() / num_samples.__float__()  # use breakpoint\n",
    "\n",
    "    print('epoch:', epoch, 'loss_value:', loss_value.item(),'Train accuracy:', acc_train.item(),'Validation accuracy:', acc_valid.item())\n",
    "\n",
    "# This part is out of train loop and it is for Test data\n",
    " # for test you do not need loss_value.backward\n",
    "yp=model(x_test)\n",
    "num_corrects = torch.sum(torch.max(yp, 1)[1] == y_test)  # use breakpoint\n",
    "acc_test= num_corrects.float() / float(num_samples)  # use breakpoint\n",
    "print('\\n Test accuracy:', acc_test.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
